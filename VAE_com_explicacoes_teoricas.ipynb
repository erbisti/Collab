{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786be138",
   "metadata": {},
   "source": [
    "# Geração de Imagens com VAE Pré-Treinado (CPU)\n",
    "Este notebook demonstra como usar um modelo de **Autoencoder Variacional (VAE)** já treinado, especificamente o `AutoencoderKL` utilizado no **Stable Diffusion**, para gerar imagens a partir de vetores latentes.\n",
    "\n",
    "O modelo utilizado foi publicado pela equipe da Stability AI e é acessível através da biblioteca `diffusers`.\n",
    "\n",
    "A execução será feita na **CPU**, para garantir compatibilidade com ambientes sem GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b4f17b",
   "metadata": {},
   "source": [
    "## O que são as bibliotecas utilizadas?\n",
    "- `torch` (PyTorch): Biblioteca de machine learning para criação e execução de redes neurais.\n",
    "- `torchvision`: Biblioteca complementar ao PyTorch, usada para manipular imagens, carregar datasets e gerar grids de imagens.\n",
    "- `diffusers`: Biblioteca da Hugging Face que oferece acesso fácil a modelos de geração de imagens como diffusion models, VAEs e outros.\n",
    "- `accelerate`: Backend utilizado pela `diffusers` para configurar o ambiente de execução (GPU/CPU, paralelismo, etc).\n",
    "- `matplotlib`: Biblioteca de visualização usada para exibir as imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers accelerate torch torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad659081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481d18d",
   "metadata": {},
   "source": [
    "## O que é um AutoencoderKL?\n",
    "- É uma variante de Autoencoder Variacional usada para codificar imagens em um espaço latente de distribuição normal.\n",
    "- `AutoencoderKL` é a implementação do VAE utilizado internamente pelo modelo **Stable Diffusion**.\n",
    "- Ao contrário de um autoencoder clássico, ele aprende não apenas um ponto, mas uma **distribuição de vetores latentes**.\n",
    "- Esse modelo foi treinado para reconstruir imagens com alta fidelidade a partir desses vetores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo VAE na CPU\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float32)\n",
    "vae = vae.to(\"cpu\")\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc50dd",
   "metadata": {},
   "source": [
    "## Geração de imagens com vetores aleatórios no espaço latente\n",
    "Criamos vetores `z` com distribuição normal padrão (ruído), e os passamos pelo decoder do VAE para gerar imagens.\n",
    "O espaço latente tem dimensão `[batch, 4, 32, 32]`, ou seja, 4 canais com 32x32 cada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 8\n",
    "z = torch.randn(batch, 4, 32, 32)\n",
    "with torch.no_grad():\n",
    "    imgs = vae.decode(z).sample\n",
    "imgs = (imgs * 0.5 + 0.5).clamp(0,1)\n",
    "grid = make_grid(imgs, nrow=4)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.title(\"Imagens geradas com VAE pré-treinado (CPU)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ee550",
   "metadata": {},
   "source": [
    "## Manipulação direta do vetor latente\n",
    "Aqui criamos um vetor `z` com todos os valores zerados e ativamos um ponto específico com valor alto (`3.0`).\n",
    "Isso nos permite explorar como partes específicas do vetor latente afetam a imagem gerada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(batch, 4, 32, 32)\n",
    "z[:, :, 16, 16] = 3.0\n",
    "with torch.no_grad():\n",
    "    imgs = vae.decode(z).sample\n",
    "imgs = (imgs * 0.5 + 0.5).clamp(0,1)\n",
    "grid = make_grid(imgs, nrow=4)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.title(\"Imagens com ponto latente [16,16] ativado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d9e8d",
   "metadata": {},
   "source": [
    "## Interpolação entre dois vetores latentes\n",
    "Interpolamos entre dois vetores `z0` e `z1` com pesos crescentes, e observamos a transição visual entre eles.\n",
    "Esse tipo de operação revela como o espaço latente é **suave** e permite **variações contínuas** nas imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = torch.randn(1, 4, 32, 32)\n",
    "z1 = torch.randn(1, 4, 32, 32)\n",
    "steps = 8\n",
    "interpolated = []\n",
    "for alpha in torch.linspace(0, 1, steps):\n",
    "    z_interp = (1 - alpha) * z0 + alpha * z1\n",
    "    with torch.no_grad():\n",
    "        img = vae.decode(z_interp).sample\n",
    "        interpolated.append((img * 0.5 + 0.5).clamp(0, 1))\n",
    "imgs_interp = torch.cat(interpolated, dim=0)\n",
    "grid = make_grid(imgs_interp, nrow=steps)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.title(\"Interpolação no espaço latente entre dois vetores z\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}