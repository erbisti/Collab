{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36824f9e",
   "metadata": {},
   "source": [
    "# Treinamento de uma GAN com MNIST\n",
    "Este notebook executa uma GAN simples para geração de dígitos MNIST, com separação didática por blocos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0342d",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a069374",
   "metadata": {},
   "source": [
    "## Carregamento e Pré-processamento do MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6edfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc87a5",
   "metadata": {},
   "source": [
    "## Definição do Gerador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 100\n",
    "\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7*7*256, use_bias=False, input_shape=(latent_dim,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "    model.add(Conv2DTranspose(128, (5,5), strides=(2,2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2DTranspose(1, (5,5), strides=(1,1), padding='same', use_bias=False, activation='tanh'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131737e",
   "metadata": {},
   "source": [
    "## Definição do Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28,28,1]))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd96a5",
   "metadata": {},
   "source": [
    "## Funções de Treinamento para o Discriminador e o Gerador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_discriminator(images):\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "    generated_images = generator(noise, training=True)\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        disc_loss_real = cross_entropy(tf.ones_like(real_output) * 0.9, real_output)  # Label smoothing\n",
    "        disc_loss_fake = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        disc_loss = disc_loss_real + disc_loss_fake\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return disc_loss\n",
    "\n",
    "@tf.function\n",
    "def train_generator():\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    return gen_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339700a",
   "metadata": {},
   "source": [
    "## Funções de Visualização de Imagens Geradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_show_all_images(generator, epoch):\n",
    "    noise = tf.random.normal([25, latent_dim])\n",
    "    gen_imgs = generator(noise, training=False)\n",
    "    gen_imgs = (gen_imgs + 1) / 2.0\n",
    "    plt.figure(figsize=(5,5))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.imshow(gen_imgs[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f'Epoch {epoch+1}: Exemplos de imagens geradas')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_and_show_filtered_images(generator, discriminator, epoch, threshold=0.5):\n",
    "    noise = tf.random.normal([200, latent_dim])\n",
    "    generated_images = generator(noise, training=False)\n",
    "    predictions = discriminator(generated_images, training=False).numpy().flatten()\n",
    "    selected_indices = np.where(predictions > threshold)[0]\n",
    "    selected_images = generated_images.numpy()[selected_indices]\n",
    "\n",
    "    if len(selected_images) == 0:\n",
    "        print(f'Epoch {epoch+1}: Nenhuma imagem enganou o Discriminador nesta época.')\n",
    "        return []\n",
    "\n",
    "    selected_images = (selected_images + 1) / 2.0\n",
    "    total = len(selected_images)\n",
    "    cols = 5\n",
    "    rows = (total // cols) + 1\n",
    "    plt.figure(figsize=(cols*2, rows*2))\n",
    "    for i in range(total):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(selected_images[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f'Epoch {epoch+1}: Imagens que enganaram o Discriminador (threshold={threshold})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return selected_images\n",
    "\n",
    "def show_all_filtered_images(images):\n",
    "    if len(images) == 0:\n",
    "        print(\"Nenhuma imagem enganou o Discriminador em nenhuma época.\")\n",
    "        return\n",
    "    images = np.array(images)\n",
    "    images = (images + 1) / 2.0\n",
    "    total = len(images)\n",
    "    cols = 5\n",
    "    rows = (total // cols) + 1\n",
    "    plt.figure(figsize=(cols*2, rows*2))\n",
    "    for i in range(total):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(images[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle('Todas as imagens que enganaram o Discriminador ao longo das épocas')\n",
    "    plt.figtext(0.5, 0.01, 'Estas são todas as imagens que em algum momento durante as épocas de treinamento conseguiram enganar o Discriminador (D(x) > threshold).', ha='center', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e1944",
   "metadata": {},
   "source": [
    "## Loop Principal de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4cc273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "buffer_size = 10000\n",
    "max_batches_per_epoch = 100\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size).batch(batch_size)\n",
    "filtered_images_all_epochs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_count = 0\n",
    "    for image_batch in dataset:\n",
    "        d_loss = train_discriminator(image_batch)\n",
    "        g_loss1 = train_generator()\n",
    "        g_loss2 = train_generator()\n",
    "        batch_count += 1\n",
    "        if batch_count >= max_batches_per_epoch:\n",
    "            break\n",
    "    print(f'Epoch {epoch+1}: D loss: {d_loss.numpy():.4f}, G loss 1: {g_loss1.numpy():.4f}, G loss 2: {g_loss2.numpy():.4f}')\n",
    "    generate_and_show_all_images(generator, epoch)\n",
    "    filtered_images = generate_and_show_filtered_images(generator, discriminator, epoch, threshold=0.5)\n",
    "    if len(filtered_images) > 0:\n",
    "        filtered_images_all_epochs.extend(filtered_images)\n",
    "\n",
    "show_all_filtered_images(filtered_images_all_epochs)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
