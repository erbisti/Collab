# **Isolation Forest (2D)**


## **Geração de dados com 3 clusters normais + 10 outliers artificiais**

## make_blobs gera um conjunto de dados sintéticos agrupados,
## úteis para simulações de aprendizado não supervisionado.

## Ao passar centers=3, estamos solicitando 3 grupos (clusters).

# **IMPORTANTE:**
## - Cada grupo será centrado em um ponto aleatório no espaço,
##   com coordenadas sorteadas uniformemente entre -10 e 10
##   (esse intervalo não é documentado, mas é o padrão interno).

## - Cada ponto gerado em torno de um centro segue uma
##   distribuição normal (gaussiana) com desvio padrão definido
##   por 'cluster_std'. No exemplo abaixo, cluster_std=0.6 indica
##   que os dados estarão concentrados aproximadamente em um raio
##   de até ~1.8 unidades do centro (±3 desvios padrão).

## - Os dados gerados são bidimensionais por padrão (n_features=2),
##   ou seja, cada ponto tem duas coordenadas: X₁ e X₂.

## - O parâmetro 'random_state' fixa a semente do gerador aleatório,
##   garantindo reprodutibilidade do experimento.

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs



X_normal, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=7)
X_outliers = np.random.uniform(low=-6, high=6, size=(10, 2))
X = np.vstack([X_normal, X_outliers])

# ------------------------------
# Classe do nó de uma árvore de isolamento
# ------------------------------

class IsolationTreeNode:
    def __init__(self, depth=0, max_depth=None):
        self.left = None
        self.right = None
        self.split_feature = None
        self.split_value = None
        self.depth = depth
        self.max_depth = max_depth
        self.n_samples = None

    def fit(self, X):
        self.n_samples = X.shape[0]
        if self.depth >= self.max_depth or self.n_samples <= 1:
            return

        # Escolhe uma variável aleatória e um valor de corte aleatório
        self.split_feature = np.random.randint(0, X.shape[1])
        min_val, max_val = X[:, self.split_feature].min(), X[:, self.split_feature].max()
        if min_val == max_val:
            return
        self.split_value = np.random.uniform(min_val, max_val)

        # Separa os dados em dois subconjuntos
        left_mask = X[:, self.split_feature] < self.split_value
        X_left = X[left_mask]
        X_right = X[~left_mask]

        # Cria nós filhos recursivamente
        self.left = IsolationTreeNode(depth=self.depth + 1, max_depth=self.max_depth)
        self.left.fit(X_left)

        self.right = IsolationTreeNode(depth=self.depth + 1, max_depth=self.max_depth)
        self.right.fit(X_right)

# ------------------------------
# Função auxiliar: profundidade de caminho + fator de normalização
# ------------------------------

def path_length(x, node):
    if node.left is None or node.right is None:
        if node.n_samples <= 1:
            return node.depth
        return node.depth + c_factor(node.n_samples)
    
    if x[node.split_feature] < node.split_value:
        return path_length(x, node.left)
    else:
        return path_length(x, node.right)

def c_factor(n):
    if n <= 1:
        return 0
    return 2.0 * (np.log(n - 1) + 0.5772156649) - (2.0 * (n - 1) / n)

# ------------------------------
# Classe da floresta de isolamento
# ------------------------------

class IsolationForest:
    def __init__(self, n_estimators=100, max_samples=256, max_depth=None):
        self.n_estimators = n_estimators
        self.max_samples = max_samples
        self.max_depth = max_depth
        self.trees = []

    def fit(self, X):
        self.trees = []
        self.sample_size = min(self.max_samples, X.shape[0])
        height_limit = int(np.ceil(np.log2(self.sample_size))) if self.max_depth is None else self.max_depth

        for _ in range(self.n_estimators):
            indices = np.random.choice(X.shape[0], self.sample_size, replace=False)
            X_subset = X[indices]
            root = IsolationTreeNode(depth=0, max_depth=height_limit)
            root.fit(X_subset)
            self.trees.append(root)

    def score_samples(self, X):
        scores = np.zeros(X.shape[0])
        for i, x in enumerate(X):
            path_lengths = [path_length(x, tree) for tree in self.trees]
            avg_path_length = np.mean(path_lengths)
            score = 2 ** (-avg_path_length / c_factor(self.sample_size))
            scores[i] = score
        return scores

# ------------------------------
# Treinamento e visualização
# ------------------------------

# Treina a floresta com os dados
model = IsolationForest(n_estimators=100, max_samples=256)
model.fit(X)

# Calcula o score de anomalia para cada ponto
scores = model.score_samples(X)

# Define limiar de outlier como os 5% com maior score
threshold = np.percentile(scores, 95)
labels = (scores >= threshold).astype(int)  # 1 = outlier

# Exibe os pontos com destaque nos outliers
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm', edgecolors='k')
plt.title("Detecção de Outliers com Isolation Forest (implementação própria)")
plt.xlabel("X₁")
plt.ylabel("X₂")
plt.grid(True)
plt.show()
